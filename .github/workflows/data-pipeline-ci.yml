name: Data Pipeline CI

on:
  push:
    branches: [main]
    paths:
      - 'data-pipeline/**'
      - 'docker/**'
      - '.github/workflows/**'
  pull_request:
    branches: [dev, main, release/*]
    paths:
      - 'data-pipeline/**'
      - 'docker/**'
      - '.github/workflows/**'

env:
  PYTHON_VERSION: "3.11"
  POETRY_VERSION: "1.7.0"

# Cancel in-progress runs for the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================================================
  # JOB 1: LINT AND FORMAT
  # Fast feedback - runs in ~30-45 seconds
  # ===========================================================================
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-lint-${{ hashFiles('data-pipeline/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-lint-

      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff==0.1.9 black==23.12.1 mypy==1.8.0

      - name: Run Ruff (linting)
        run: |
          ruff check data-pipeline/ --output-format=github
        continue-on-error: false

      - name: Run Black (formatting check)
        run: |
          black --check --diff data-pipeline/

  # ===========================================================================
  # JOB 2: UNIT TESTS
  # Core test suite with coverage enforcement
  # ===========================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-test-${{ hashFiles('data-pipeline/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-test-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          cd data-pipeline
          pip install -e ".[dev]"

      - name: Run unit tests with coverage
        run: |
          cd data-pipeline
          pytest tests/unit/ \
            -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            --tb=short \
            -x  # Stop on first failure for faster feedback
        env:
          PYTHONPATH: ${{ github.workspace }}/data-pipeline

      - name: Upload coverage report
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: data-pipeline/coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  # ===========================================================================
  # JOB 3: SCHEMA TESTS
  # Validate all schema definitions and enforcement logic
  # ===========================================================================
  schema-tests:
    name: Schema Validation Tests
    runs-on: ubuntu-latest
    needs: lint
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-schema-${{ hashFiles('data-pipeline/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-schema-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          cd data-pipeline
          pip install -e ".[dev]"

      - name: Validate schema JSON files
        run: |
          cd data-pipeline
          python -c "
          import json
          import sys
          from pathlib import Path

          schemas_dir = Path('schemas')
          if not schemas_dir.exists():
              print('No schemas directory found, skipping validation')
              sys.exit(0)

          errors = []
          for schema_file in schemas_dir.rglob('*.json'):
              try:
                  with open(schema_file) as f:
                      json.load(f)
                  print(f'✓ {schema_file}')
              except json.JSONDecodeError as e:
                  errors.append(f'✗ {schema_file}: {e}')

          if errors:
              for error in errors:
                  print(error)
              sys.exit(1)

          print(f'All schema files are valid JSON')
          "

      - name: Run schema tests
        run: |
          cd data-pipeline
          pytest tests/schema_tests/ -v --tb=short || echo "No schema tests found yet"
        env:
          PYTHONPATH: ${{ github.workspace }}/data-pipeline

  # ===========================================================================
  # JOB 4: DAG VALIDATION
  # Ensure all Airflow DAGs are syntactically correct and importable
  # ===========================================================================
  dag-validation:
    name: Airflow DAG Validation
    runs-on: ubuntu-latest
    needs: lint
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-airflow-${{ hashFiles('data-pipeline/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-airflow-

      - name: Install Airflow and dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow==2.7.3" \
                      "apache-airflow-providers-google>=10.0.0"
          cd data-pipeline
          pip install -e "." --no-deps || true  # Install local package for imports

      - name: Initialize Airflow DB
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          airflow db init
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: "false"
          AIRFLOW__CORE__UNIT_TEST_MODE: "true"

      - name: Validate DAG syntax and imports
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          export PYTHONPATH="${{ github.workspace }}/data-pipeline:$PYTHONPATH"

          python << 'EOF'
          import sys
          from pathlib import Path

          # Check if dags directory exists
          dags_dir = Path('data-pipeline/dags')
          if not dags_dir.exists():
              print('No dags directory found, skipping DAG validation')
              sys.exit(0)

          # Find all DAG files
          dag_files = list(dags_dir.rglob('*_dag.py')) + list(dags_dir.rglob('*_dag.py'))
          if not dag_files:
              print('No DAG files found, skipping validation')
              sys.exit(0)

          from airflow.models import DagBag

          dag_bag = DagBag(
              dag_folder=str(dags_dir),
              include_examples=False,
          )

          # Check for import errors
          if dag_bag.import_errors:
              print('DAG Import Errors:')
              for dag_id, error in dag_bag.import_errors.items():
                  print(f'  ✗ {dag_id}:')
                  print(f'    {error}')
              sys.exit(1)

          # List validated DAGs
          print(f'Successfully validated {len(dag_bag.dags)} DAG(s):')
          for dag_id in sorted(dag_bag.dags.keys()):
              dag = dag_bag.dags[dag_id]
              print(f'  ✓ {dag_id} ({len(dag.tasks)} tasks)')

          sys.exit(0)
          EOF
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: "false"

  # ===========================================================================
  # JOB 5: INTEGRATION TESTS
  # Full pipeline tests - only on main/dev branches
  # ===========================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, schema-tests, dag-validation]
    timeout-minutes: 30
    if: github.base_ref == 'main' || github.base_ref == 'dev'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ hashFiles('data-pipeline/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-integration-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          cd data-pipeline
          pip install -e ".[dev]"

      - name: Run integration tests
        run: |
          cd data-pipeline
          pytest tests/integration/ \
            -v \
            --timeout=300 \
            --tb=short \
            || echo "No integration tests found yet"
        env:
          PYTHONPATH: ${{ github.workspace }}/data-pipeline
          STORAGE_EMULATOR_HOST: http://localhost:4443
          BP_ENVIRONMENT: dev
